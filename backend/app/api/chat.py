from fastapi import APIRouter, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, Any
import json
from .settings import load_user_settings_to_app_state
from .auth import get_current_user
from pathlib import Path
from ..core.prompt_library import get_prompt
from ..core.logger import logprint
from langchain_core.messages import HumanMessage
from ..agent.graph import InputSchema

router = APIRouter(tags=["Chat"])


def get_app_state(request: Request):
    """Dependency to get app state"""
    return request.app.state


class ChatRequest(BaseModel):
    message: str = Field(description="The message to send to the LLM")
    system_instruction: Optional[str] = Field(
        None, description="Optional system instruction to set the behavior of the LLM"
    )
    model: str = Field(
        default="gemini-2.5-flash",
        description="The LLM model to use for the conversation",
    )


class ChatResponse(BaseModel):
    response: str = Field(description="The response generated by the LLM")
    model: str = Field(
        description="The LLM model that was used to generate the response"
    )


class DataAnalysisRequest(BaseModel):
    current_code: str = Field(
        default="",
        description="current python code which will be used to guide LLM to generate new code",
    )
    question: str = Field(description="The question to ask about the data")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use")
    context: Optional[str] = Field(
        None, description="Additional context about the data"
    )


class DataAnalysisResponse(BaseModel):
    is_safe: bool = Field(description="Whether the query is safe to execute")
    is_relevant: bool = Field(
        description="Whether the query is relevant to data analysis"
    )
    code: str = Field(description="Generated Python code for the analysis")
    explanation: str = Field(
        description="Explanation of the analysis in markdown format"
    )


def _validate_analysis_preconditions(app_state) -> None:
    if not hasattr(app_state, "api_key") or app_state.api_key is None:
        raise HTTPException(
            status_code=401, detail="API key not set. Please set your API key first."
        )

    if (
        not hasattr(app_state, "llm_initialized")
        or not app_state.llm_initialized
        or not hasattr(app_state, "llm_service")
        or app_state.llm_service is None
    ):
        raise HTTPException(
            status_code=503,
            detail="LLM service not available. Please check your API key.",
        )

    if not hasattr(app_state, "schema_path") or app_state.schema_path is None:
        raise HTTPException(
            status_code=400,
            detail="Schema path not configured. Please set your data path and context using /settings/set/filepath and /settings/set/context endpoints.",
        )

    if not hasattr(app_state, "data_path") or app_state.data_path is None:
        raise HTTPException(
            status_code=400,
            detail="Data path not configured. Please set your data path using /settings/set/filepath endpoint.",
        )


def _build_data_analysis_response(result: dict[str, Any]) -> DataAnalysisResponse:
    final_messages = result.get("messages", [])
    last_message = final_messages[-1].content if final_messages else ""

    metadata = result.get("metadata", {})
    is_safe = (
        metadata.is_safe if hasattr(metadata, "is_safe") else metadata.get("is_safe", True)
    )
    is_relevant = (
        metadata.is_relevant
        if hasattr(metadata, "is_relevant")
        else metadata.get("is_relevant", True)
    )

    code = result.get("current_code", "") or result.get("code", "")
    code_guard_feedback = result.get("code_guard_feedback", "") or ""
    if code:
        explanation = result.get("plan", "") or "Code generated based on request."
    else:
        if code_guard_feedback:
            explanation = (
                "I could not generate executable code that passed validation.\n"
                f"Reason: {code_guard_feedback}"
            )
        else:
            explanation = last_message or "No explanation provided."

    if not code and "```python" in explanation:
        import re

        code_match = re.search(r"```python\n(.*?)```", explanation, re.DOTALL)
        if code_match:
            code = code_match.group(1)

    return DataAnalysisResponse(
        is_safe=bool(is_safe),
        is_relevant=bool(is_relevant),
        code=code or "",
        explanation=str(explanation),
    )


async def _run_agent_analysis(
    request: DataAnalysisRequest,
    current_user: dict,
    app_state,
    schema: dict[str, Any],
    table_name: str,
) -> DataAnalysisResponse:
    user_msg = request.question
    logprint(f"üîç [Agent] Loaded Schema Keys: {list(schema.keys())}")
    if "cricket" in str(schema).lower():
        logprint("‚ö†Ô∏è [Agent] CRICKET DETECTED IN SCHEMA!", level="warning")

    input_state = InputSchema(
        messages=[HumanMessage(content=user_msg)],
        active_schema=schema,
        previous_code=request.current_code,
        table_name=table_name,
        data_path=app_state.data_path,
    )
    thread_id = f"{current_user['user_id']}:{app_state.data_path}"
    config = {"configurable": {"api_key": app_state.api_key, "thread_id": thread_id}}

    logprint(f"ü§ñ [Agent] Question: {user_msg}")
    result = await app_state.agent_graph.ainvoke(input_state, config=config)
    logprint(f"ü§ñ [Agent] Raw Output:\n{json.dumps(result, default=str, indent=2)}")

    response_obj = _build_data_analysis_response(result)
    logprint(f"ü§ñ [Agent] Final Response:\n{response_obj.model_dump_json(indent=2)}")
    return response_obj


async def _run_legacy_analysis(
    request: DataAnalysisRequest,
    app_state,
    schema: dict[str, Any],
    table_name: str,
) -> DataAnalysisResponse:
    system_instruction = get_prompt(
        "business_analysis_system",
        table_name=table_name,
        schema=schema,
        data_path=app_state.data_path,
        current_code=request.current_code,
    )
    app_state.llm_service.create_chat_client(
        system_instruction=system_instruction, model=request.model
    )
    user_question = get_prompt(
        "business_analysis_user", question=request.question, table_name=table_name
    )
    response = app_state.llm_service.chat(user_question)
    if response is None:
        raise HTTPException(
            status_code=500,
            detail="Failed to parse response from LLM. The response could not be structured as expected.",
        )
    return DataAnalysisResponse(**response.dict())


def _to_sse(event: str, payload: dict[str, Any]) -> str:
    return f"event: {event}\ndata: {json.dumps(payload, default=str)}\n\n"


def load_schema(filepath: str | Path) -> dict:
    path = Path(filepath)
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"Schema file not found: {path}")

    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON in schema file: {e}")
    except OSError as e:
        raise HTTPException(status_code=500, detail=f"Error reading schema file: {e}")

    if not isinstance(data, dict):
        raise HTTPException(status_code=400, detail="Schema JSON must be a JSON object")

    return data


@router.post("/chat", response_model=DataAnalysisResponse)
async def chat_endpoint(
    request: DataAnalysisRequest,
    current_user: dict = Depends(get_current_user),
    app_state=Depends(get_app_state),
):
    """
    Data analysis chat endpoint that returns structured response with code and explanation
    """
    # Load user settings into app_state if they exist
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    _validate_analysis_preconditions(app_state)

    try:
        schema = load_schema(app_state.schema_path)
        table_name = getattr(app_state, "table_name", "your_table_name")

        if hasattr(app_state, "agent_graph") and app_state.agent_graph:
            return await _run_agent_analysis(
                request=request,
                current_user=current_user,
                app_state=app_state,
                schema=schema,
                table_name=table_name,
            )
        return await _run_legacy_analysis(
            request=request, app_state=app_state, schema=schema, table_name=table_name
        )

    except HTTPException:
        # Preserve intentional HTTP status codes (e.g., 400/401/404)
        raise
    except Exception as e:
        import traceback
        logprint(f"‚ùå [Agent] Error processing analysis request:\n{traceback.format_exc()}", level="error")
        raise HTTPException(
            status_code=500, detail=f"Error processing analysis request: {str(e)}"
        )


@router.post("/chat/stream")
async def chat_stream_endpoint(
    request: DataAnalysisRequest,
    http_request: Request,
    current_user: dict = Depends(get_current_user),
    app_state=Depends(get_app_state),
):
    load_user_settings_to_app_state(current_user["user_id"], app_state)
    _validate_analysis_preconditions(app_state)
    schema = load_schema(app_state.schema_path)
    table_name = getattr(app_state, "table_name", "your_table_name")

    async def event_generator():
        try:
            yield _to_sse("status", {"stage": "start", "message": "Starting analysis"})

            if hasattr(app_state, "agent_graph") and app_state.agent_graph:
                user_msg = request.question
                input_state = InputSchema(
                    messages=[HumanMessage(content=user_msg)],
                    active_schema=schema,
                    previous_code=request.current_code,
                    table_name=table_name,
                    data_path=app_state.data_path,
                )
                thread_id = f"{current_user['user_id']}:{app_state.data_path}"
                config = {
                    "configurable": {
                        "api_key": app_state.api_key,
                        "thread_id": thread_id,
                    }
                }

                aggregated: dict[str, Any] = {}
                async for step in app_state.agent_graph.astream(input_state, config=config):
                    if await http_request.is_disconnected():
                        return
                    for node_name, payload in step.items():
                        if isinstance(payload, dict):
                            aggregated.update(payload)
                        if node_name == "code_guard":
                            logprint(
                                "[Agent][Stream] code_guard node",
                                level="INFO",
                                request_id=thread_id,
                                guard_status=aggregated.get("guard_status"),
                                retry_count=aggregated.get("code_guard_retries", 0),
                                feedback=aggregated.get("code_guard_feedback", ""),
                            )
                        yield _to_sse(
                            "node",
                            {"node": node_name, "message": f"{node_name} completed"},
                        )

                # If checkpointer is enabled, state has the canonical merged output.
                try:
                    final_state = await app_state.agent_graph.aget_state(config)
                    if getattr(final_state, "values", None):
                        aggregated = final_state.values
                except Exception:
                    pass

                final_response = _build_data_analysis_response(aggregated)
                logprint(
                    "[Agent][Stream] final response summary",
                    level="INFO",
                    request_id=thread_id,
                    has_code=bool((final_response.code or "").strip()),
                    code_len=len(final_response.code or ""),
                    has_await_query=("await query(" in (final_response.code or "")),
                    guard_status=aggregated.get("guard_status", ""),
                    guard_feedback=aggregated.get("code_guard_feedback", ""),
                )
                yield _to_sse("final", final_response.model_dump())
                return

            final_response = await _run_legacy_analysis(
                request=request, app_state=app_state, schema=schema, table_name=table_name
            )
            yield _to_sse("final", final_response.model_dump())
        except HTTPException as e:
            yield _to_sse("error", {"detail": e.detail, "status_code": e.status_code})
        except Exception as e:
            import traceback

            logprint(
                f"‚ùå [Agent] Stream error processing analysis request:\n{traceback.format_exc()}",
                level="error",
            )
            yield _to_sse("error", {"detail": str(e), "status_code": 500})

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={"Cache-Control": "no-cache", "Connection": "keep-alive"},
    )

@router.get("/history")
async def get_chat_history(
    current_user: dict = Depends(get_current_user),
    app_state=Depends(get_app_state),
):
    """
    Get chat history for the current user and active data file.
    """
    # Load user settings into app_state so we have the correct data_path
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    if not hasattr(app_state, "agent_graph") or not app_state.agent_graph:
        return {"messages": [], "current_code": ""}

    # Ensure data path is set to identify the thread
    if not hasattr(app_state, "data_path") or app_state.data_path is None:
        # If no data path is set, we can't retrieve history
        return {"messages": [], "current_code": ""}

    thread_id = f"{current_user['user_id']}:{app_state.data_path}"
    config = {"configurable": {"thread_id": thread_id}}

    try:
        # Get the state from the graph
        state = await app_state.agent_graph.aget_state(config)
        
        if not state.values:
            return {"messages": [], "current_code": ""}
            
        messages = state.values.get("messages", [])
        current_code = state.values.get("current_code", "")
        
        # Process messages into Q&A pairs
        # The state contains: HumanMessage, AIMessage (safety), AIMessage (relevancy), ...
        # We want to pair each HumanMessage with a single meaningful response
        
        # Get plan from state - this is the best explanation when code was generated
        plan = state.values.get("plan", "")
        code = state.values.get("code") or state.values.get("current_code", "")
        metadata = state.values.get("metadata", {})
        
        # Handle metadata as dict or object
        if hasattr(metadata, "model_dump"):
            metadata = metadata.model_dump()
        elif hasattr(metadata, "dict"):
            metadata = metadata.dict()
        
        # Build aggregated history - one entry per user question
        aggregated_history = []
        
        # Group messages: each user message followed by all AI messages until next user
        i = 0
        while i < len(messages):
            msg = messages[i]
            
            if isinstance(msg, HumanMessage):
                user_content = msg.content
                if isinstance(user_content, list):
                    user_content = "\n".join([str(p) for p in user_content])
                
                # Collect all subsequent AI messages until next HumanMessage
                ai_messages = []
                j = i + 1
                while j < len(messages) and not isinstance(messages[j], HumanMessage):
                    ai_msg = messages[j]
                    content = ai_msg.content
                    if isinstance(content, list):
                        content = "\n".join([str(p) for p in content])
                    ai_messages.append(str(content))
                    j += 1
                
                # Determine the best explanation for this Q&A pair
                # Check if this is the LAST user message (we have plan/code for it)
                is_last_question = (j >= len(messages))
                
                if is_last_question and plan:
                    # Use the plan as explanation for the last question
                    explanation = plan
                elif is_last_question and not plan and ai_messages:
                    # No plan, use last AI message (could be general_purpose or noncode response)
                    explanation = ai_messages[-1] if ai_messages else ""
                elif ai_messages:
                    # For older questions, use the last AI message in the group
                    # This is imperfect but better than showing intermediate reasoning
                    explanation = ai_messages[-1]
                else:
                    explanation = ""
                
                # Add user message
                aggregated_history.append({
                    "role": "user",
                    "content": str(user_content),
                    "type": "human"
                })
                
                # Add assistant response if we have one
                if explanation:
                    aggregated_history.append({
                        "role": "assistant", 
                        "content": explanation,
                        "type": "ai"
                    })
                
                i = j  # Move to next user message
            else:
                # Skip orphaned AI messages at the start
                i += 1

        return {
            "messages": aggregated_history,
            "current_code": current_code
        }
    except Exception as e:
        logprint(f"Error fetching chat history: {e}", level="error")
        return {"messages": [], "current_code": ""}
