from fastapi import APIRouter, HTTPException, Request, Depends
from pydantic import BaseModel, Field
from typing import Optional
import json
from .settings import load_user_settings_to_app_state
from .auth import get_current_user
from pathlib import Path
from pathlib import Path
from pathlib import Path
from ..prompt_library import get_prompt
from ..logger import logprint
from langchain_core.messages import HumanMessage
from ..agent.graph import InputSchema
from langchain_core.runnables import RunnableConfig

router = APIRouter(tags=["Chat"])


def get_app_state(request: Request):
    """Dependency to get app state"""
    return request.app.state


class ChatRequest(BaseModel):
    message: str = Field(description="The message to send to the LLM")
    system_instruction: Optional[str] = Field(
        None, description="Optional system instruction to set the behavior of the LLM"
    )
    model: str = Field(
        default="gemini-2.5-flash",
        description="The LLM model to use for the conversation",
    )


class ChatResponse(BaseModel):
    response: str = Field(description="The response generated by the LLM")
    model: str = Field(
        description="The LLM model that was used to generate the response"
    )


class DataAnalysisRequest(BaseModel):
    current_code: str = Field(
        default="",
        description="current python code which will be used to guide LLM to generate new code",
    )
    question: str = Field(description="The question to ask about the data")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use")
    context: Optional[str] = Field(
        None, description="Additional context about the data"
    )


class DataAnalysisResponse(BaseModel):
    is_safe: bool = Field(description="Whether the query is safe to execute")
    is_relevant: bool = Field(
        description="Whether the query is relevant to data analysis"
    )
    code: str = Field(description="Generated Python code for the analysis")
    explanation: str = Field(
        description="Explanation of the analysis in markdown format"
    )


def load_schema(filepath: str | Path) -> dict:
    path = Path(filepath)
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"Schema file not found: {path}")

    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON in schema file: {e}")
    except OSError as e:
        raise HTTPException(status_code=500, detail=f"Error reading schema file: {e}")

    if not isinstance(data, dict):
        raise HTTPException(status_code=400, detail="Schema JSON must be a JSON object")

    return data


@router.post("/chat", response_model=DataAnalysisResponse)
async def chat_endpoint(
    request: DataAnalysisRequest,
    current_user: dict = Depends(get_current_user),
    app_state=Depends(get_app_state),
):
    """
    Data analysis chat endpoint that returns structured response with code and explanation
    """
    # Load user settings into app_state if they exist
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    if not hasattr(app_state, "api_key") or app_state.api_key is None:
        raise HTTPException(
            status_code=401, detail="API key not set. Please set your API key first."
        )

    if (
        not hasattr(app_state, "llm_initialized")
        or not app_state.llm_initialized
        or not hasattr(app_state, "llm_service")
        or app_state.llm_service is None
    ):
        raise HTTPException(
            status_code=503,
            detail="LLM service not available. Please check your API key.",
        )

    try:
        # Check if required settings are available
        if not hasattr(app_state, "schema_path") or app_state.schema_path is None:
            raise HTTPException(
                status_code=400,
                detail="Schema path not configured. Please set your data path and context using /settings/set/filepath and /settings/set/context endpoints.",
            )

        if not hasattr(app_state, "data_path") or app_state.data_path is None:
            raise HTTPException(
                status_code=400,
                detail="Data path not configured. Please set your data path using /settings/set/filepath endpoint.",
            )

        # Build context from user settings stored in app_state
        schema = load_schema(app_state.schema_path)

        # Get table name from app state
        table_name = getattr(app_state, "table_name", "your_table_name")

        # --- LANGGRAPH AGENT INTEGRATION ---
        if hasattr(app_state, "agent_graph") and app_state.agent_graph:
            # Prepare input for the agent
            user_msg = request.question
            
            # Create input state
            logprint(f"üîç [Agent] Loaded Schema Keys: {list(schema.keys())}")
            # Identify if this looks like cricket data
            if "cricket" in str(schema).lower():
                logprint("‚ö†Ô∏è [Agent] CRICKET DETECTED IN SCHEMA!", level="warning")
            
            input_state = InputSchema(
                messages=[HumanMessage(content=user_msg)],
                active_schema=schema,
                previous_code=request.current_code, # Map user's current editor content to previous_code
                table_name=table_name,
                data_path=app_state.data_path
            )
            
            # Invoke agent
            # Pass API key via configurable for dynamic model initialization
            api_key = app_state.api_key
            # Invoke agent
            # Pass API key via configurable for dynamic model initialization
            api_key = app_state.api_key
            
            # Use user_id + data_path as thread_id for persistence
            thread_id = f"{current_user['user_id']}:{app_state.data_path}"
            
            config = {
                "configurable": {
                    "api_key": api_key,
                    "thread_id": thread_id
                }
            }
            
            logprint(f"ü§ñ [Agent] Question: {user_msg}")
            result = await app_state.agent_graph.ainvoke(input_state, config=config)
            
            # Log raw output as requested
            logprint(f"ü§ñ [Agent] Raw Output:\n{json.dumps(result, default=str, indent=2)}")
            
            # Extract results from state
            final_messages = result.get("messages", [])
            last_message = final_messages[-1].content if final_messages else ""
            
            metadata = result.get("metadata", {})
            # metadata is a MetaData object or dict
            is_safe = metadata.is_safe if hasattr(metadata, "is_safe") else metadata.get("is_safe", True)
            is_relevant = metadata.is_relevant if hasattr(metadata, "is_relevant") else metadata.get("is_relevant", True)
            
            code = result.get("current_code", "") or result.get("code", "")
            
            if code:
                # If code is present, use 'plan' as the explanation
                explanation = result.get("plan", "") or "Code generated based on request."
            else:
                # If no code, combine reasoning or use the last message
                is_safe_reason = metadata.safety_reasoning if hasattr(metadata, "safety_reasoning") else metadata.get("safety_reasoning", "")
                is_relevant_reason = metadata.relevancy_reasoning if hasattr(metadata, "relevancy_reasoning") else metadata.get("relevancy_reasoning", "")
                
                parts = []
                if is_safe_reason:
                    parts.append(f"Safety Analysis: {is_safe_reason}")
                if is_relevant_reason:
                    parts.append(f"Relevancy Analysis: {is_relevant_reason}")
                
                if parts:
                    explanation = "\n\n".join(parts)
                else:
                    # Fallback to the last message if no metadata reasoning is available (e.g. general chit-chat)
                    explanation = last_message or "No explanation provided."
            
            # Clean up potential code blocks in explanation if they leaked
            if not code and "```python" in explanation:
                 import re
                 code_match = re.search(r"```python\n(.*?)```", explanation, re.DOTALL)
                 if code_match:
                     code = code_match.group(1)
            
            response_obj = DataAnalysisResponse(
                is_safe=bool(is_safe),
                is_relevant=bool(is_relevant),
                code=code or "",
                explanation=str(explanation)
            )
            
            logprint(f"ü§ñ [Agent] Final Response:\n{response_obj.model_dump_json(indent=2)}")
            return response_obj
            
        # --- FALLBACK TO LEGACY LLM SERVICE ---

        # Create system instruction for business analysis
        system_instruction = get_prompt(
            "business_analysis_system",
            table_name=table_name,
            schema=schema,
            data_path=app_state.data_path,
            current_code=request.current_code,
        )

        # Create chat client with data analysis instruction
        app_state.llm_service.create_chat_client(
            system_instruction=system_instruction, model=request.model
        )

        # Format the user question
        user_question = get_prompt(
            "business_analysis_user", question=request.question, table_name=table_name
        )

        # Get response from LLM
        response = app_state.llm_service.chat(user_question)

        if response is None:
            raise HTTPException(
                status_code=500,
                detail="Failed to parse response from LLM. The response could not be structured as expected.",
            )

        # response is a CodeOutput instance, convert to DataAnalysisResponse
        return DataAnalysisResponse(**response.dict())

    except Exception as e:
        import traceback
        logprint(f"‚ùå [Agent] Error processing analysis request:\n{traceback.format_exc()}", level="error")
        raise HTTPException(
            status_code=500, detail=f"Error processing analysis request: {str(e)}"
        )

@router.get("/history")
async def get_chat_history(
    current_user: dict = Depends(get_current_user),
    app_state=Depends(get_app_state),
):
    """
    Get chat history for the current user and active data file.
    """
    # Load user settings into app_state so we have the correct data_path
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    if not hasattr(app_state, "agent_graph") or not app_state.agent_graph:
        return {"messages": [], "current_code": ""}

    # Ensure data path is set to identify the thread
    if not hasattr(app_state, "data_path") or app_state.data_path is None:
        # If no data path is set, we can't retrieve history
        return {"messages": [], "current_code": ""}

    thread_id = f"{current_user['user_id']}:{app_state.data_path}"
    config = {"configurable": {"thread_id": thread_id}}

    try:
        # Get the state from the graph
        state = await app_state.agent_graph.aget_state(config)
        
        if not state.values:
            return {"messages": [], "current_code": ""}
            
        messages = state.values.get("messages", [])
        current_code = state.values.get("current_code", "")
        
        # Serialize messages to simple dict format for frontend
        history = []
        for msg in messages:
            role = "user" if isinstance(msg, HumanMessage) else "assistant"
            # Extract content - handle string or list of content parts
            content = msg.content
            if isinstance(content, list):
                # Simple join for now, improve if needed for multimodal
                content = "\\n".join([str(p) for p in content])
            


            history.append({
                "role": role,
                "content": str(content),
                "type": msg.type
            })
            
        # Attempt to reconstruct the rich explanation for the LAST assistant message
        # because the 'plan' or 'metadata' in state corresponds to the latest interaction.
        if history and history[-1]["role"] == "assistant":
            # Logic mirrored from chat_endpoint to reconstruct 'explanation'
            plan = state.values.get("plan")
            code = state.values.get("code") or state.values.get("current_code")
            metadata = state.values.get("metadata", {})
            
            # Handle metadata as dict or object
            if hasattr(metadata, "dict"):
                 metadata = metadata.dict()
            elif hasattr(metadata, "model_dump"):
                 metadata = metadata.model_dump()
            
            # Reconstruct explanation
            explanation = ""
            if plan:
                explanation = plan
            else:
                is_safe_reason = metadata.get("safety_reasoning", "")
                is_relevant_reason = metadata.get("relevancy_reasoning", "")
                
                parts = []
                if is_safe_reason:
                    parts.append(f"Safety Analysis: {is_safe_reason}")
                if is_relevant_reason:
                    parts.append(f"Relevancy Analysis: {is_relevant_reason}")
                
                if parts:
                    explanation = "\n\n".join(parts)
            
            # If we successfully reconstructed a better explanation, use it
            if explanation:
                history[-1]["content"] = explanation

        # Aggregate consecutive assistant messages to simplify frontend handling
        aggregated_history = []
        for msg in history:
            if aggregated_history and msg["role"] == "assistant" and aggregated_history[-1]["role"] == "assistant":
                # Append to previous message
                aggregated_history[-1]["content"] += "\n\n" + msg["content"]
            else:
                # Start new message
                aggregated_history.append(msg)

        return {
            "messages": aggregated_history,
            "current_code": current_code
        }
    except Exception as e:
        logprint(f"Error fetching chat history: {e}", level="error")
        return {"messages": [], "current_code": ""}
