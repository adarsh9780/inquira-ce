from fastapi import APIRouter, HTTPException, Request, Depends
from pydantic import BaseModel, Field
from typing import Optional
import json
from .settings import load_user_settings_to_app_state
from .auth import get_current_user
from pathlib import Path
from ..prompt_library import get_prompt

router = APIRouter(tags=["Chat"])

def get_app_state(request: Request):
    """Dependency to get app state"""
    return request.app.state

class ChatRequest(BaseModel):
    message: str = Field(description="The message to send to the LLM")
    system_instruction: Optional[str] = Field(None, description="Optional system instruction to set the behavior of the LLM")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use for the conversation")

class ChatResponse(BaseModel):
    response: str = Field(description="The response generated by the LLM")
    model: str = Field(description="The LLM model that was used to generate the response")

class DataAnalysisRequest(BaseModel):
    current_code: str = Field(default='', description="current python code which will be used to guide LLM to generate new code")
    question: str = Field(description="The question to ask about the data")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use")
    context: Optional[str] = Field(None, description="Additional context about the data")

class DataAnalysisResponse(BaseModel):
    is_safe: bool = Field(description="Whether the query is safe to execute")
    is_relevant: bool = Field(description="Whether the query is relevant to data analysis")
    code: str = Field(description="Generated Python code for the analysis")
    explanation: str = Field(description="Explanation of the analysis in markdown format")


def load_schema(filepath: str | Path) -> dict:
    path = Path(filepath)
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"Schema file not found: {path}")

    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON in schema file: {e}")
    except OSError as e:
        raise HTTPException(status_code=500, detail=f"Error reading schema file: {e}")

    if not isinstance(data, dict):
        raise HTTPException(status_code=400, detail="Schema JSON must be a JSON object")

    return data

@router.post("/chat", response_model=DataAnalysisResponse)
async def chat_endpoint(
    request: DataAnalysisRequest,
    current_user: dict = Depends(get_current_user),
    app_state = Depends(get_app_state)
):
    """
    Data analysis chat endpoint that returns structured response with code and explanation
    """
    # Load user settings into app_state if they exist
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    if not hasattr(app_state, 'api_key') or app_state.api_key is None:
        raise HTTPException(
            status_code=401,
            detail="API key not set. Please set your API key first."
        )

    if not hasattr(app_state, 'llm_initialized') or not app_state.llm_initialized or not hasattr(app_state, 'llm_service') or app_state.llm_service is None:
        raise HTTPException(
            status_code=503,
            detail="LLM service not available. Please check your API key."
        )

    try:
        # Check if required settings are available
        if not hasattr(app_state, 'schema_path') or app_state.schema_path is None:
            raise HTTPException(
                status_code=400,
                detail="Schema path not configured. Please set your data path and context using /settings/set/filepath and /settings/set/context endpoints."
            )

        if not hasattr(app_state, 'data_path') or app_state.data_path is None:
            raise HTTPException(
                status_code=400,
                detail="Data path not configured. Please set your data path using /settings/set/filepath endpoint."
            )

        # Build context from user settings stored in app_state
        schema = load_schema(app_state.schema_path)

        # Get table name from app state
        table_name = getattr(app_state, 'table_name', 'your_table_name')

        # Create system instruction for business analysis
        system_instruction = get_prompt(
            "business_analysis_system",
            table_name=table_name,
            schema=schema,
            data_path=app_state.data_path,
            current_code=request.current_code
        )

        # Create chat client with data analysis instruction
        app_state.llm_service.create_chat_client(
            system_instruction=system_instruction,
            model=request.model
        )

        # Format the user question
        user_question = get_prompt(
            "business_analysis_user",
            question=request.question,
            table_name=table_name
        )

        # Get response from LLM
        response = app_state.llm_service.chat(user_question)

        if response is None:
            raise HTTPException(
                status_code=500,
                detail="Failed to parse response from LLM. The response could not be structured as expected."
            )

        # response is a CodeOutput instance, convert to DataAnalysisResponse
        return DataAnalysisResponse(**response.dict())

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing analysis request: {str(e)}")