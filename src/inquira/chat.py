from fastapi import APIRouter, HTTPException, Request, Depends
from pydantic import BaseModel, Field
from typing import Optional
import json
from .settings import load_user_settings_to_app_state
from .auth import get_current_user
from pathlib import Path

router = APIRouter(tags=["Chat"])

def get_app_state(request: Request):
    """Dependency to get app state"""
    return request.app.state

class ChatRequest(BaseModel):
    message: str = Field(description="The message to send to the LLM")
    system_instruction: Optional[str] = Field(None, description="Optional system instruction to set the behavior of the LLM")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use for the conversation")

class ChatResponse(BaseModel):
    response: str = Field(description="The response generated by the LLM")
    model: str = Field(description="The LLM model that was used to generate the response")

class DataAnalysisRequest(BaseModel):
    current_code: str = Field(default='', description="current python code which will be used to guide LLM to generate new code")
    question: str = Field(description="The question to ask about the data")
    model: str = Field(default="gemini-2.5-flash", description="The LLM model to use")
    context: Optional[str] = Field(None, description="Additional context about the data")

class DataAnalysisResponse(BaseModel):
    is_safe: bool = Field(description="Whether the query is safe to execute")
    is_relevant: bool = Field(description="Whether the query is relevant to the data")
    code: str = Field(description="Generated Python code for the analysis")
    explanation: str = Field(description="Explanation of the analysis in markdown format")


def load_schema(filepath: str | Path) -> dict:
    path = Path(filepath)
    if not path.exists():
        raise HTTPException(status_code=404, detail=f"Schema file not found: {path}")

    try:
        with path.open("r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON in schema file: {e}")
    except OSError as e:
        raise HTTPException(status_code=500, detail=f"Error reading schema file: {e}")

    if not isinstance(data, dict):
        raise HTTPException(status_code=400, detail="Schema JSON must be a JSON object")

    return data

@router.post("/chat", response_model=DataAnalysisResponse)
async def chat_endpoint(
    request: DataAnalysisRequest,
    current_user: dict = Depends(get_current_user),
    app_state = Depends(get_app_state)
):
    """
    Data analysis chat endpoint that returns structured response with code and explanation
    """
    # Load user settings into app_state if they exist
    load_user_settings_to_app_state(current_user["user_id"], app_state)

    if not hasattr(app_state, 'api_key') or app_state.api_key is None:
        raise HTTPException(
            status_code=401,
            detail="API key not set. Please set your API key first."
        )

    if not hasattr(app_state, 'llm_initialized') or not app_state.llm_initialized or not hasattr(app_state, 'llm_service') or app_state.llm_service is None:
        raise HTTPException(
            status_code=503,
            detail="LLM service not available. Please check your API key."
        )

    try:
        # Check if required settings are available
        if not hasattr(app_state, 'schema_path') or app_state.schema_path is None:
            raise HTTPException(
                status_code=400,
                detail="Schema path not configured. Please set your settings first using /settings/create endpoint."
            )

        if not hasattr(app_state, 'data_path') or app_state.data_path is None:
            raise HTTPException(
                status_code=400,
                detail="Data path not configured. Please set your settings first using /settings/create endpoint."
            )

        # Build context from user settings stored in app_state
        schema = load_schema(app_state.schema_path)

        # Create system instruction for data analysis
        system_instruction = """
        You are an expert data analysis assistant. When given a question you should:

        1. Determine if the question is safe to answer (is_safe)
        2. Determine if the question is relevant to data analysis (is_relevant)
        3. Generate appropriate Python code to answer the question (code)
        4. Provide a clear explanation in markdown format (explanation)

        The code should be generated based on the following schema:
        {schema}

        Occasionally, you might also see an existing code which will help you to understand what the user was trying.
        If there is code, build on top of it, making sure that the new code is self sufficient, meaning it should be able to produce
        the user ask without user needing to run the old code.
        {code}

        Prefer to read only as much as data as required to do the analysis (avoid loading whole data into memory, no matter what).
        Use DuckDB to achieve this. The data is stored in the following location:
        {data_path}
        Once the required data is loaded by duckdb, feel free to use Pandas to achieve the final result.

        # Variables to create in the code
        If required, generate a Plotly figure and store it in a variable called **figure**.
        Default output will be a pandas dataframe and should be stored in variable called **result_df**.

        Return your response as a JSON object with these exact keys:
        - is_safe: boolean
        - is_relevant: boolean
        - code: string (Python code)
        - explanation: string (markdown formatted explanation) - step by step markdown format explanantion of what code is doing to acheive the task
        """

        # Create chat client with data analysis instruction
        app_state.llm_service.create_chat_client(
            system_instruction=system_instruction.format(schema=schema, data_path=app_state.data_path, code=request.current_code),
            model=request.model
        )

        # Format the user question
        user_question = f"""
        Please analyze this question and provide a structured response:

        Question: {request.question}

        Remember to return only a valid JSON object with is_safe, is_relevant, code, and explanation keys.

        # Special Instruction
        - use regular expression `regexp_matches` in duckdb to filter out rows based on string type columns
        """

        # Get response from LLM
        response = app_state.llm_service.chat(user_question)

        if response is None:
            raise HTTPException(
                status_code=500,
                detail="Failed to parse response from LLM. The response could not be structured as expected."
            )

        # response is a CodeOutput instance, convert to DataAnalysisResponse
        return DataAnalysisResponse(**response.dict())

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing analysis request: {str(e)}")
