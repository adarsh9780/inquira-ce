# Inquira Configuration
# =====================
# User-editable settings for Python environment and proxy configuration.
# This file is read by the Tauri launcher on startup.

[python]
# Python version to install via UV (only used on first launch)
version = "3.12"

# Set this if you're behind a corporate proxy or using JFrog/Nexus/Artifactory
# index-url = "https://your-company.jfrog.io/artifactory/api/pypi/pypi-remote/simple"
# Environment override (higher priority than this file): INQUIRA_UV_INDEX_URL

# Set this to use a pre-installed Python instead of UV-managed one
# python-path = "/usr/local/bin/python3.12"

[proxy]
# Uncomment and set these if you're behind a corporate proxy
# http-proxy = "http://proxy.company.com:8080"
# https-proxy = "http://proxy.company.com:8080"
# no-proxy = "localhost,127.0.0.1"

[backend]
# Port for the FastAPI backend server
port = 8000
# Host to bind the backend server to
host = "localhost"

[execution]
# Switch execution backend without code changes.
# Supported values: "local_jupyter", "local_subprocess"
provider = "local_jupyter"

[execution.runner]
# Name of the dedicated code-runner virtual environment folder under app data (~/.inquira/*)
venv-name = ".runner-venv"
# Optional package list installed into the runner venv alongside ipykernel and defaults.
packages = ["narwhals", "duckdb", "pandas", "plotly", "pyarrow", "polars"]
# Idle kernel eviction threshold in minutes.
kernel-idle-minutes = 30
# Runtime timeout guardrails.
timeout-seconds = 60
memory-limit-mb = 512
max-output-kb = 512

[backend.phoenix]
# Enable Phoenix tracing for LangGraph/LangChain (used by backend startup)
enabled = false
# Phoenix project name shown in UI
project = "inquira-dev"
# Local Phoenix collector endpoint (OTLP HTTP)
endpoint = "http://127.0.0.1:6006/v1/traces"

[llm]
# OpenAI-compatible runtime provider settings.
provider = "openrouter"
base-url = "https://openrouter.ai/api/v1"
default-model = "google/gemini-2.5-flash"
lite-model = "google/gemini-2.5-flash-lite"
# Frontend model dropdown and server-side allowlist; use full model IDs.
models = [
  "google/gemini-3-flash-preview",
  "google/gemini-2.5-flash",
  "google/gemini-2.5-flash-lite",
  "openrouter/free",
]

[llm.limits]
# Default output cap when no endpoint-specific override is used.
default = 4096
# Schema generation/regeneration cap.
schema = 2048
# Code-generation/chat execution cap.
code_generation = 4096
